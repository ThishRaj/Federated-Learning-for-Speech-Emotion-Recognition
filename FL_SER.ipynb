{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOSj-3BmXr3U",
        "outputId": "9f1972eb-84e8-4085-f993-f31289991d12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qvi_l9XKMsu2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dense\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrj3acKrCUcN"
      },
      "source": [
        "# Instructions to run this code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltU_9n1vCUcO"
      },
      "source": [
        "To run it from your machine/gdrive, please change path to the location of this .ipynb file, and store the following files generated by `preprocessing.ipynb` in the same folder:\n",
        "- x_train.npy\n",
        "- y_train.npy\n",
        "- x_test.npy\n",
        "- y_test.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1-ByXL8WAFE6"
      },
      "outputs": [],
      "source": [
        "path = '/content/gdrive/My Drive/Colab Notebooks/EE627A Term Project/Ravdess/' # change this path according to your needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0O4xODFrc7oO"
      },
      "outputs": [],
      "source": [
        "X_train = np.load(path+'x_train.npy')\n",
        "X_test = np.load(path+'x_test.npy')\n",
        "\n",
        "y_train = np.load(path+'y_train.npy')\n",
        "y_test = np.load(path+'y_test.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KQEL-H0hgd95"
      },
      "outputs": [],
      "source": [
        "x_traincnn = np.expand_dims(X_train, axis=2)\n",
        "x_testcnn = np.expand_dims(X_test, axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xSuyh5h6Mtyu"
      },
      "outputs": [],
      "source": [
        "def clients(speech_list, labels, clients_number, intl='client'):\n",
        "    client_name = ['{}_{}'.format(intl, i+1) for i in range(clients_number)]\n",
        "    data = list(zip(speech_list, labels))\n",
        "    size = len(data)//clients_number\n",
        "    data_segments = [data[i:i + size] for i in range(0, size*clients_number, size)]\n",
        "    assert(len(data_segments) == len(client_name))\n",
        "    return {client_name[i] : data_segments[i] for i in range(len(client_name))} "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ukntoawBMt5r"
      },
      "outputs": [],
      "source": [
        "num_K = 10\n",
        "clients = clients(x_traincnn, y_train, num_K, intl='client')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8ZWCnVWQMt8K"
      },
      "outputs": [],
      "source": [
        "def batch_data(data_segments, batch_size):\n",
        "    data, label = zip(*data_segments)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
        "    return dataset.shuffle(len(label)).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ihLNBftzMt-_"
      },
      "outputs": [],
      "source": [
        "B = 20\n",
        "batch_client = dict()\n",
        "for (client_name, data) in clients.items():\n",
        "    batch_client[client_name] = batch_data(data,B)\n",
        "    \n",
        "test_batched = tf.data.Dataset.from_tensor_slices((x_traincnn, y_train)).batch(len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2dvdskFBNjJ"
      },
      "source": [
        "# Convolutional Neural Network for Speech Emotion Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SNqWSVs75dV8"
      },
      "outputs": [],
      "source": [
        "class SER_CNN:\n",
        "    @staticmethod\n",
        "    def build(x_traincnn, y_train):\n",
        "        model = Sequential()\n",
        "        model.add(Conv1D(256, 5,padding='same', input_shape=(x_traincnn.shape[1],1), name = 'Conv1D_1', activation = 'relu'))\n",
        "        model.add(Conv1D(128, 5,padding='same',name='Conv1D_2',activation='relu'))\n",
        "        model.add(Dropout(0.1,name = 'Dropout_1'))\n",
        "        model.add(MaxPooling1D(pool_size=(8), name = 'MaxPooling1D_1'))\n",
        "        model.add(Conv1D(128, 5,padding='same',activation='relu',name = 'Conv1D_3'))\n",
        "        model.add(Conv1D(128, 5, padding=\"valid\",activation='relu',name='Conv1D_4'))\n",
        "        model.add(Flatten(name = 'Flatten_1'))\n",
        "        model.add(Dense(4,activation ='softmax',name= 'Dense_1'))\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "f_FErrx6eK12"
      },
      "outputs": [],
      "source": [
        "comms_round = 200\n",
        "loss='categorical_crossentropy'\n",
        "metrics = ['accuracy',tf.keras.metrics.Recall(),tf.keras.metrics.Precision() ]\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.00001, decay=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7XjYg4GzeK4P"
      },
      "outputs": [],
      "source": [
        "def scaling_factor(clients_data, client): \n",
        "    name_client = list(clients_data.keys())\n",
        "    bs = list(clients_data[client])[0][0].shape[0]\n",
        "    count_global = sum([tf.data.experimental.cardinality(clients_data[client]).numpy() for client in name_client])*bs\n",
        "    count_local = tf.data.experimental.cardinality(clients_data[client]).numpy()*bs\n",
        "    return count_local/count_global"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_weights(wght, var):\n",
        "\n",
        "    weight = []\n",
        "    step = len(wght)\n",
        "    for i in range(step):\n",
        "        weight.append(var * wght[i])\n",
        "    return weight"
      ],
      "metadata": {
        "id": "ZIXY8Nn0WrLc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_weights(weight_list):\n",
        "    \n",
        "    avg = list()\n",
        "    for tuple_list in zip(*weight_list):\n",
        "        mean = tf.math.reduce_sum(tuple_list, axis=0)\n",
        "        avg.append(mean)   \n",
        "    return avg"
      ],
      "metadata": {
        "id": "NpKhL9SpWwKq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(X_test, Y_test,  model, comm_round):\n",
        "\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    logits = model.predict(X_test, batch_size=32)\n",
        "    loss = cce(Y_test, logits)\n",
        "    acc =  accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    m = tf.keras.metrics.Recall()\n",
        "    m.update_state(Y_test, logits)\n",
        "    recall = m.result()\n",
        "    return acc, loss, recall.numpy()"
      ],
      "metadata": {
        "id": "B90ohDF_Wy0C"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBGpcpvMfJ7L",
        "outputId": "8b1a0734-9aa1-4d8a-f63c-652a3287e831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Conv1D_1 (Conv1D)           (None, 97, 256)           1536      \n",
            "                                                                 \n",
            " Conv1D_2 (Conv1D)           (None, 97, 128)           163968    \n",
            "                                                                 \n",
            " Dropout_1 (Dropout)         (None, 97, 128)           0         \n",
            "                                                                 \n",
            " MaxPooling1D_1 (MaxPooling1  (None, 12, 128)          0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " Conv1D_3 (Conv1D)           (None, 12, 128)           82048     \n",
            "                                                                 \n",
            " Conv1D_4 (Conv1D)           (None, 8, 128)            82048     \n",
            "                                                                 \n",
            " Flatten_1 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " Dense_1 (Dense)             (None, 4)                 4100      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 333,700\n",
            "Trainable params: 333,700\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "comm_round: 0 | global_acc: 29.713% | global_recall 0.000% | global_loss: 1.3876996040344238\n",
            "comm_round: 1 | global_acc: 34.766% | global_recall 0.000% | global_loss: 1.3739433288574219\n",
            "comm_round: 2 | global_acc: 34.961% | global_recall 0.000% | global_loss: 1.3709218502044678\n",
            "comm_round: 3 | global_acc: 35.732% | global_recall 0.000% | global_loss: 1.3665244579315186\n",
            "comm_round: 4 | global_acc: 38.072% | global_recall 0.000% | global_loss: 1.3644983768463135\n",
            "comm_round: 5 | global_acc: 39.623% | global_recall 0.000% | global_loss: 1.362471103668213\n",
            "comm_round: 6 | global_acc: 40.209% | global_recall 0.000% | global_loss: 1.359437346458435\n",
            "comm_round: 7 | global_acc: 41.565% | global_recall 0.000% | global_loss: 1.3562591075897217\n",
            "comm_round: 8 | global_acc: 43.509% | global_recall 0.000% | global_loss: 1.3531506061553955\n",
            "comm_round: 9 | global_acc: 46.424% | global_recall 0.000% | global_loss: 1.351460337638855\n",
            "comm_round: 10 | global_acc: 47.199% | global_recall 0.000% | global_loss: 1.3487354516983032\n",
            "comm_round: 11 | global_acc: 47.783% | global_recall 0.000% | global_loss: 1.3463962078094482\n",
            "comm_round: 12 | global_acc: 49.340% | global_recall 0.195% | global_loss: 1.3437705039978027\n",
            "comm_round: 13 | global_acc: 49.335% | global_recall 0.195% | global_loss: 1.3410569429397583\n",
            "comm_round: 14 | global_acc: 51.672% | global_recall 0.389% | global_loss: 1.3392152786254883\n",
            "comm_round: 15 | global_acc: 51.476% | global_recall 0.389% | global_loss: 1.335706353187561\n",
            "comm_round: 16 | global_acc: 52.059% | global_recall 0.389% | global_loss: 1.333197832107544\n",
            "comm_round: 17 | global_acc: 52.824% | global_recall 0.971% | global_loss: 1.3313863277435303\n",
            "comm_round: 18 | global_acc: 52.627% | global_recall 1.361% | global_loss: 1.3289884328842163\n",
            "comm_round: 19 | global_acc: 53.603% | global_recall 1.166% | global_loss: 1.3266286849975586\n",
            "comm_round: 20 | global_acc: 53.800% | global_recall 1.166% | global_loss: 1.3235108852386475\n",
            "comm_round: 21 | global_acc: 53.800% | global_recall 1.361% | global_loss: 1.3217190504074097\n",
            "comm_round: 22 | global_acc: 55.157% | global_recall 2.530% | global_loss: 1.3177450895309448\n",
            "comm_round: 23 | global_acc: 54.575% | global_recall 2.724% | global_loss: 1.3163388967514038\n",
            "comm_round: 24 | global_acc: 54.769% | global_recall 2.918% | global_loss: 1.3142740726470947\n",
            "comm_round: 25 | global_acc: 55.546% | global_recall 4.085% | global_loss: 1.3115042448043823\n",
            "comm_round: 26 | global_acc: 55.543% | global_recall 5.060% | global_loss: 1.309029459953308\n",
            "comm_round: 27 | global_acc: 55.155% | global_recall 6.029% | global_loss: 1.3061637878417969\n",
            "comm_round: 28 | global_acc: 57.484% | global_recall 7.773% | global_loss: 1.3021372556686401\n",
            "comm_round: 29 | global_acc: 56.318% | global_recall 7.967% | global_loss: 1.3010411262512207\n",
            "comm_round: 30 | global_acc: 57.287% | global_recall 8.939% | global_loss: 1.297614574432373\n",
            "comm_round: 31 | global_acc: 57.484% | global_recall 9.325% | global_loss: 1.2959609031677246\n",
            "comm_round: 32 | global_acc: 58.261% | global_recall 10.491% | global_loss: 1.2923474311828613\n",
            "comm_round: 33 | global_acc: 57.872% | global_recall 10.878% | global_loss: 1.289629578590393\n",
            "comm_round: 34 | global_acc: 58.648% | global_recall 11.847% | global_loss: 1.287680745124817\n",
            "comm_round: 35 | global_acc: 58.648% | global_recall 12.433% | global_loss: 1.2852513790130615\n",
            "comm_round: 36 | global_acc: 57.095% | global_recall 13.021% | global_loss: 1.2826800346374512\n",
            "comm_round: 37 | global_acc: 57.865% | global_recall 13.993% | global_loss: 1.2802276611328125\n",
            "comm_round: 38 | global_acc: 59.620% | global_recall 13.990% | global_loss: 1.2778162956237793\n",
            "comm_round: 39 | global_acc: 58.841% | global_recall 15.548% | global_loss: 1.2744665145874023\n",
            "comm_round: 40 | global_acc: 59.808% | global_recall 15.354% | global_loss: 1.273026704788208\n",
            "comm_round: 41 | global_acc: 60.200% | global_recall 16.129% | global_loss: 1.2700166702270508\n",
            "comm_round: 42 | global_acc: 58.253% | global_recall 16.324% | global_loss: 1.2688237428665161\n",
            "comm_round: 43 | global_acc: 60.583% | global_recall 17.684% | global_loss: 1.2656092643737793\n",
            "comm_round: 44 | global_acc: 60.002% | global_recall 19.045% | global_loss: 1.2623893022537231\n",
            "comm_round: 45 | global_acc: 59.807% | global_recall 19.822% | global_loss: 1.2597413063049316\n",
            "comm_round: 46 | global_acc: 61.166% | global_recall 19.238% | global_loss: 1.2590885162353516\n",
            "comm_round: 47 | global_acc: 60.391% | global_recall 20.011% | global_loss: 1.256956934928894\n",
            "comm_round: 48 | global_acc: 61.748% | global_recall 20.597% | global_loss: 1.2541759014129639\n",
            "comm_round: 49 | global_acc: 62.137% | global_recall 20.986% | global_loss: 1.2526355981826782\n",
            "comm_round: 50 | global_acc: 62.138% | global_recall 21.374% | global_loss: 1.2504311800003052\n",
            "comm_round: 51 | global_acc: 60.971% | global_recall 22.343% | global_loss: 1.247028112411499\n",
            "comm_round: 52 | global_acc: 61.361% | global_recall 22.149% | global_loss: 1.245779275894165\n",
            "comm_round: 53 | global_acc: 60.583% | global_recall 23.127% | global_loss: 1.2446473836898804\n",
            "comm_round: 54 | global_acc: 62.526% | global_recall 23.507% | global_loss: 1.2419453859329224\n",
            "comm_round: 55 | global_acc: 62.140% | global_recall 23.120% | global_loss: 1.2402081489562988\n",
            "comm_round: 56 | global_acc: 62.524% | global_recall 25.068% | global_loss: 1.238046646118164\n",
            "comm_round: 57 | global_acc: 62.723% | global_recall 24.475% | global_loss: 1.2362140417099\n",
            "comm_round: 58 | global_acc: 62.523% | global_recall 25.256% | global_loss: 1.2336626052856445\n",
            "comm_round: 59 | global_acc: 62.524% | global_recall 26.036% | global_loss: 1.2322027683258057\n",
            "comm_round: 60 | global_acc: 63.301% | global_recall 26.036% | global_loss: 1.2304558753967285\n",
            "comm_round: 61 | global_acc: 63.107% | global_recall 26.619% | global_loss: 1.2291206121444702\n",
            "comm_round: 62 | global_acc: 63.301% | global_recall 27.200% | global_loss: 1.2266474962234497\n",
            "comm_round: 63 | global_acc: 61.754% | global_recall 27.780% | global_loss: 1.226030707359314\n",
            "comm_round: 64 | global_acc: 63.301% | global_recall 27.977% | global_loss: 1.224595546722412\n",
            "comm_round: 65 | global_acc: 63.499% | global_recall 28.560% | global_loss: 1.22301185131073\n",
            "comm_round: 66 | global_acc: 64.078% | global_recall 28.170% | global_loss: 1.2218668460845947\n",
            "comm_round: 67 | global_acc: 64.273% | global_recall 28.170% | global_loss: 1.2204582691192627\n",
            "comm_round: 68 | global_acc: 64.468% | global_recall 29.142% | global_loss: 1.2179601192474365\n",
            "comm_round: 69 | global_acc: 63.882% | global_recall 29.139% | global_loss: 1.2176207304000854\n",
            "comm_round: 70 | global_acc: 64.270% | global_recall 30.305% | global_loss: 1.2147526741027832\n",
            "comm_round: 71 | global_acc: 63.884% | global_recall 31.274% | global_loss: 1.2135930061340332\n",
            "comm_round: 72 | global_acc: 63.689% | global_recall 30.693% | global_loss: 1.2126567363739014\n",
            "comm_round: 73 | global_acc: 63.689% | global_recall 31.467% | global_loss: 1.2118144035339355\n",
            "comm_round: 74 | global_acc: 63.885% | global_recall 32.439% | global_loss: 1.2094714641571045\n",
            "comm_round: 75 | global_acc: 64.464% | global_recall 31.274% | global_loss: 1.2093174457550049\n",
            "comm_round: 76 | global_acc: 65.045% | global_recall 32.049% | global_loss: 1.2082912921905518\n",
            "comm_round: 77 | global_acc: 64.270% | global_recall 32.632% | global_loss: 1.2065470218658447\n",
            "comm_round: 78 | global_acc: 64.658% | global_recall 32.439% | global_loss: 1.2058343887329102\n",
            "comm_round: 79 | global_acc: 63.882% | global_recall 33.215% | global_loss: 1.2043075561523438\n",
            "comm_round: 80 | global_acc: 65.439% | global_recall 32.440% | global_loss: 1.20404851436615\n",
            "comm_round: 81 | global_acc: 65.240% | global_recall 33.601% | global_loss: 1.2023968696594238\n",
            "comm_round: 82 | global_acc: 65.239% | global_recall 33.797% | global_loss: 1.2014201879501343\n",
            "comm_round: 83 | global_acc: 65.240% | global_recall 33.990% | global_loss: 1.2002520561218262\n",
            "comm_round: 84 | global_acc: 64.859% | global_recall 33.992% | global_loss: 1.1991832256317139\n",
            "comm_round: 85 | global_acc: 65.053% | global_recall 33.411% | global_loss: 1.199065089225769\n",
            "comm_round: 86 | global_acc: 65.631% | global_recall 34.378% | global_loss: 1.196897029876709\n",
            "comm_round: 87 | global_acc: 66.217% | global_recall 33.992% | global_loss: 1.1964014768600464\n",
            "comm_round: 88 | global_acc: 64.854% | global_recall 34.378% | global_loss: 1.1955591440200806\n",
            "comm_round: 89 | global_acc: 66.023% | global_recall 34.767% | global_loss: 1.1940456628799438\n",
            "comm_round: 90 | global_acc: 66.215% | global_recall 35.349% | global_loss: 1.1931183338165283\n",
            "comm_round: 91 | global_acc: 65.633% | global_recall 35.352% | global_loss: 1.1924926042556763\n",
            "comm_round: 92 | global_acc: 65.631% | global_recall 36.519% | global_loss: 1.1916306018829346\n",
            "comm_round: 93 | global_acc: 66.214% | global_recall 36.125% | global_loss: 1.190312147140503\n",
            "comm_round: 94 | global_acc: 66.019% | global_recall 36.905% | global_loss: 1.18864905834198\n",
            "comm_round: 95 | global_acc: 66.211% | global_recall 36.711% | global_loss: 1.1888115406036377\n",
            "comm_round: 96 | global_acc: 66.020% | global_recall 36.319% | global_loss: 1.187914490699768\n",
            "comm_round: 97 | global_acc: 66.219% | global_recall 35.736% | global_loss: 1.1877048015594482\n",
            "comm_round: 98 | global_acc: 66.405% | global_recall 37.100% | global_loss: 1.1868401765823364\n",
            "comm_round: 99 | global_acc: 66.212% | global_recall 36.711% | global_loss: 1.1861683130264282\n",
            "comm_round: 100 | global_acc: 65.825% | global_recall 37.488% | global_loss: 1.1842710971832275\n",
            "comm_round: 101 | global_acc: 66.798% | global_recall 37.486% | global_loss: 1.1837334632873535\n",
            "comm_round: 102 | global_acc: 66.409% | global_recall 36.514% | global_loss: 1.1840230226516724\n",
            "comm_round: 103 | global_acc: 67.188% | global_recall 38.068% | global_loss: 1.18287992477417\n",
            "comm_round: 104 | global_acc: 66.603% | global_recall 37.488% | global_loss: 1.1824933290481567\n",
            "comm_round: 105 | global_acc: 67.578% | global_recall 37.094% | global_loss: 1.1823246479034424\n",
            "comm_round: 106 | global_acc: 66.794% | global_recall 37.682% | global_loss: 1.181122064590454\n",
            "comm_round: 107 | global_acc: 66.797% | global_recall 38.455% | global_loss: 1.1804009675979614\n",
            "comm_round: 108 | global_acc: 67.381% | global_recall 37.482% | global_loss: 1.1806020736694336\n",
            "comm_round: 109 | global_acc: 66.992% | global_recall 38.649% | global_loss: 1.1791658401489258\n",
            "comm_round: 110 | global_acc: 66.797% | global_recall 38.843% | global_loss: 1.178121566772461\n",
            "comm_round: 111 | global_acc: 66.798% | global_recall 38.455% | global_loss: 1.1786788702011108\n",
            "comm_round: 112 | global_acc: 66.602% | global_recall 39.232% | global_loss: 1.1765726804733276\n",
            "comm_round: 113 | global_acc: 66.992% | global_recall 39.038% | global_loss: 1.1762263774871826\n",
            "comm_round: 114 | global_acc: 66.798% | global_recall 39.618% | global_loss: 1.1756372451782227\n",
            "comm_round: 115 | global_acc: 66.797% | global_recall 39.424% | global_loss: 1.175083875656128\n",
            "comm_round: 116 | global_acc: 66.795% | global_recall 39.232% | global_loss: 1.1743977069854736\n",
            "comm_round: 117 | global_acc: 66.409% | global_recall 39.232% | global_loss: 1.1735665798187256\n",
            "comm_round: 118 | global_acc: 67.769% | global_recall 41.173% | global_loss: 1.173421859741211\n",
            "comm_round: 119 | global_acc: 67.186% | global_recall 39.620% | global_loss: 1.173412561416626\n",
            "comm_round: 120 | global_acc: 67.577% | global_recall 39.813% | global_loss: 1.172733187675476\n",
            "comm_round: 121 | global_acc: 67.577% | global_recall 40.393% | global_loss: 1.172668218612671\n",
            "comm_round: 122 | global_acc: 67.770% | global_recall 40.007% | global_loss: 1.1718052625656128\n",
            "comm_round: 123 | global_acc: 67.575% | global_recall 41.561% | global_loss: 1.1708123683929443\n",
            "comm_round: 124 | global_acc: 67.577% | global_recall 40.395% | global_loss: 1.1707019805908203\n",
            "comm_round: 125 | global_acc: 67.770% | global_recall 40.395% | global_loss: 1.170365333557129\n",
            "comm_round: 126 | global_acc: 67.963% | global_recall 42.144% | global_loss: 1.1689730882644653\n",
            "comm_round: 127 | global_acc: 67.383% | global_recall 41.170% | global_loss: 1.1694157123565674\n",
            "comm_round: 128 | global_acc: 66.994% | global_recall 40.975% | global_loss: 1.1693748235702515\n",
            "comm_round: 129 | global_acc: 67.188% | global_recall 40.782% | global_loss: 1.1688109636306763\n",
            "comm_round: 130 | global_acc: 67.381% | global_recall 41.364% | global_loss: 1.1671979427337646\n",
            "comm_round: 131 | global_acc: 68.158% | global_recall 40.589% | global_loss: 1.166639804840088\n",
            "comm_round: 132 | global_acc: 67.964% | global_recall 40.975% | global_loss: 1.167309284210205\n",
            "comm_round: 133 | global_acc: 67.966% | global_recall 41.750% | global_loss: 1.1666429042816162\n",
            "comm_round: 134 | global_acc: 67.577% | global_recall 41.947% | global_loss: 1.1657058000564575\n",
            "comm_round: 135 | global_acc: 67.380% | global_recall 41.365% | global_loss: 1.1652804613113403\n",
            "comm_round: 136 | global_acc: 67.964% | global_recall 41.558% | global_loss: 1.1643469333648682\n",
            "comm_round: 137 | global_acc: 67.575% | global_recall 42.920% | global_loss: 1.1636903285980225\n",
            "comm_round: 138 | global_acc: 68.938% | global_recall 41.945% | global_loss: 1.164338231086731\n",
            "comm_round: 139 | global_acc: 68.353% | global_recall 42.533% | global_loss: 1.1631104946136475\n",
            "comm_round: 140 | global_acc: 68.160% | global_recall 42.528% | global_loss: 1.1625608205795288\n",
            "comm_round: 141 | global_acc: 67.964% | global_recall 43.116% | global_loss: 1.1618428230285645\n",
            "comm_round: 142 | global_acc: 68.938% | global_recall 42.334% | global_loss: 1.1619327068328857\n",
            "comm_round: 143 | global_acc: 68.741% | global_recall 41.947% | global_loss: 1.161473035812378\n",
            "comm_round: 144 | global_acc: 68.741% | global_recall 42.334% | global_loss: 1.161238670349121\n",
            "comm_round: 145 | global_acc: 68.549% | global_recall 42.139% | global_loss: 1.1617498397827148\n",
            "comm_round: 146 | global_acc: 68.745% | global_recall 42.919% | global_loss: 1.1606934070587158\n",
            "comm_round: 147 | global_acc: 69.322% | global_recall 42.722% | global_loss: 1.1599829196929932\n",
            "comm_round: 148 | global_acc: 69.132% | global_recall 42.528% | global_loss: 1.1600697040557861\n",
            "comm_round: 149 | global_acc: 69.130% | global_recall 43.110% | global_loss: 1.1588722467422485\n",
            "comm_round: 150 | global_acc: 68.935% | global_recall 44.278% | global_loss: 1.1576656103134155\n",
            "comm_round: 151 | global_acc: 68.547% | global_recall 43.500% | global_loss: 1.1579720973968506\n",
            "comm_round: 152 | global_acc: 68.741% | global_recall 43.497% | global_loss: 1.1567959785461426\n",
            "comm_round: 153 | global_acc: 68.547% | global_recall 43.110% | global_loss: 1.1569950580596924\n",
            "comm_round: 154 | global_acc: 68.742% | global_recall 45.635% | global_loss: 1.1555463075637817\n",
            "comm_round: 155 | global_acc: 68.935% | global_recall 43.497% | global_loss: 1.1557555198669434\n",
            "comm_round: 156 | global_acc: 69.714% | global_recall 43.305% | global_loss: 1.1560803651809692\n",
            "comm_round: 157 | global_acc: 68.744% | global_recall 43.497% | global_loss: 1.1553990840911865\n",
            "comm_round: 158 | global_acc: 68.356% | global_recall 44.082% | global_loss: 1.1561905145645142\n",
            "comm_round: 159 | global_acc: 68.549% | global_recall 44.472% | global_loss: 1.1543221473693848\n",
            "comm_round: 160 | global_acc: 68.745% | global_recall 44.278% | global_loss: 1.1539334058761597\n",
            "comm_round: 161 | global_acc: 69.521% | global_recall 44.085% | global_loss: 1.1550264358520508\n",
            "comm_round: 162 | global_acc: 68.938% | global_recall 44.666% | global_loss: 1.1533937454223633\n",
            "comm_round: 163 | global_acc: 68.939% | global_recall 45.055% | global_loss: 1.152928352355957\n",
            "comm_round: 164 | global_acc: 68.552% | global_recall 45.441% | global_loss: 1.151900291442871\n",
            "comm_round: 165 | global_acc: 69.328% | global_recall 44.861% | global_loss: 1.1520106792449951\n",
            "comm_round: 166 | global_acc: 69.133% | global_recall 44.857% | global_loss: 1.1525660753250122\n",
            "comm_round: 167 | global_acc: 68.941% | global_recall 45.443% | global_loss: 1.1515412330627441\n",
            "comm_round: 168 | global_acc: 69.133% | global_recall 44.274% | global_loss: 1.1515026092529297\n",
            "comm_round: 169 | global_acc: 69.713% | global_recall 44.666% | global_loss: 1.1509267091751099\n",
            "comm_round: 170 | global_acc: 69.716% | global_recall 45.051% | global_loss: 1.151048183441162\n",
            "comm_round: 171 | global_acc: 69.130% | global_recall 45.052% | global_loss: 1.1495411396026611\n",
            "comm_round: 172 | global_acc: 69.521% | global_recall 46.023% | global_loss: 1.148526668548584\n",
            "comm_round: 173 | global_acc: 69.522% | global_recall 45.443% | global_loss: 1.149695634841919\n",
            "comm_round: 174 | global_acc: 69.716% | global_recall 45.443% | global_loss: 1.1494356393814087\n",
            "comm_round: 175 | global_acc: 69.135% | global_recall 46.218% | global_loss: 1.1489715576171875\n",
            "comm_round: 176 | global_acc: 68.938% | global_recall 45.049% | global_loss: 1.1497009992599487\n",
            "comm_round: 177 | global_acc: 69.908% | global_recall 45.249% | global_loss: 1.148134469985962\n",
            "comm_round: 178 | global_acc: 69.910% | global_recall 45.830% | global_loss: 1.148173451423645\n",
            "comm_round: 179 | global_acc: 70.296% | global_recall 45.440% | global_loss: 1.147881031036377\n",
            "comm_round: 180 | global_acc: 69.327% | global_recall 45.632% | global_loss: 1.148145318031311\n",
            "comm_round: 181 | global_acc: 70.297% | global_recall 45.830% | global_loss: 1.1470386981964111\n",
            "comm_round: 182 | global_acc: 69.327% | global_recall 46.604% | global_loss: 1.147274136543274\n",
            "comm_round: 183 | global_acc: 70.105% | global_recall 46.993% | global_loss: 1.146681308746338\n",
            "comm_round: 184 | global_acc: 70.104% | global_recall 45.635% | global_loss: 1.1457209587097168\n",
            "comm_round: 185 | global_acc: 70.491% | global_recall 46.216% | global_loss: 1.145196557044983\n",
            "comm_round: 186 | global_acc: 69.716% | global_recall 46.798% | global_loss: 1.145801067352295\n",
            "comm_round: 187 | global_acc: 69.524% | global_recall 47.187% | global_loss: 1.1446471214294434\n",
            "comm_round: 188 | global_acc: 70.491% | global_recall 46.605% | global_loss: 1.1447474956512451\n",
            "comm_round: 189 | global_acc: 70.297% | global_recall 46.024% | global_loss: 1.144576907157898\n",
            "comm_round: 190 | global_acc: 70.490% | global_recall 47.188% | global_loss: 1.1438976526260376\n",
            "comm_round: 191 | global_acc: 70.296% | global_recall 46.799% | global_loss: 1.1439510583877563\n",
            "comm_round: 192 | global_acc: 70.882% | global_recall 45.829% | global_loss: 1.1438283920288086\n",
            "comm_round: 193 | global_acc: 70.491% | global_recall 46.607% | global_loss: 1.142927885055542\n",
            "comm_round: 194 | global_acc: 70.104% | global_recall 47.578% | global_loss: 1.142460823059082\n",
            "comm_round: 195 | global_acc: 70.299% | global_recall 47.187% | global_loss: 1.142602801322937\n",
            "comm_round: 196 | global_acc: 70.102% | global_recall 46.993% | global_loss: 1.1426533460617065\n",
            "comm_round: 197 | global_acc: 70.296% | global_recall 46.995% | global_loss: 1.1420531272888184\n",
            "comm_round: 198 | global_acc: 70.685% | global_recall 47.188% | global_loss: 1.1404932737350464\n",
            "comm_round: 199 | global_acc: 69.913% | global_recall 47.379% | global_loss: 1.1417700052261353\n"
          ]
        }
      ],
      "source": [
        "SER_CNN_global = SER_CNN()\n",
        "global_model = SER_CNN_global.build(x_traincnn, y_train)\n",
        "E = 5\n",
        "global_model.summary()\n",
        "\n",
        "history_acc = []\n",
        "history_loss = []\n",
        "history_recall = []\n",
        "    \n",
        "\n",
        "for comm_round in range(comms_round):\n",
        "            \n",
        "\n",
        "    global_weights = global_model.get_weights()\n",
        "    \n",
        "    \n",
        "    scaled_local_weight_list = list()\n",
        "\n",
        "    \n",
        "    client_names= list(batch_client.keys())\n",
        "    random.shuffle(client_names)\n",
        "    \n",
        "    \n",
        "    for client in client_names:\n",
        "        SER_CNN_local = SER_CNN()\n",
        "        local_model = SER_CNN_local.build(x_traincnn, y_train)\n",
        "        local_model.compile(loss=loss, \n",
        "                      optimizer=optimizer, \n",
        "                      metrics=metrics)\n",
        "        \n",
        "        \n",
        "        local_model.set_weights(global_weights)\n",
        "        \n",
        "        \n",
        "        local_model.fit(batch_client[client], epochs=E, verbose=0) \n",
        "        \n",
        "        \n",
        "        scaling_factor_obj = scaling_factor(batch_client, client)\n",
        "        scaled_weights = model_weights(local_model.get_weights(), scaling_factor_obj)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "        \n",
        "        \n",
        "        K.clear_session()\n",
        "        \n",
        "    \n",
        "    average_weights = sum_weights(scaled_local_weight_list)\n",
        "    \n",
        "    \n",
        "    global_model.set_weights(average_weights)\n",
        "\n",
        "    \n",
        "    average_acc = 0\n",
        "    average_loss = 0\n",
        "    average_recall = 0\n",
        "    count = 0\n",
        "\n",
        "    for(x_testcnn, y_test) in test_batched:\n",
        "        global_acc, global_loss, global_recall = test_model(x_testcnn, y_test, global_model, comm_round)\n",
        "        average_acc += global_acc\n",
        "        average_loss += global_loss\n",
        "        average_recall += global_recall\n",
        "        count+=1\n",
        "    average_acc = average_acc/count\n",
        "    average_recall = average_recall/count\n",
        "    average_loss = average_loss/count\n",
        "    print('comm_round: {} | global_acc: {:.3%} | global_recall {:.3%} | global_loss: {}'.format(comm_round, average_acc, average_recall, average_loss))\n",
        "    history_acc.append(average_acc)\n",
        "    history_loss.append(average_loss)\n",
        "    history_recall.append(average_recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the results\n",
        "To save the results for a particular configuration change the file name (by changing the * with a number) below. The results will get stored as follows:\n",
        "- history_acc*.npy - stores the training accuracy\n",
        "- history_recall*.npy - stores the training recall\n",
        "- history_loss*.npy - stores the training loss\n",
        "\n",
        "\n",
        "The test configurations and the corresponding file numbers are as shown in the table below - \n",
        "\n",
        "|Sl. No. | Clients No.(K) | Batch Size(B) | Epochs(E) | File No.|\n",
        "|--------|:---------------|:--------------|:----------|:--------|\n",
        "|    1   |        5       |     10        |    1      |    1    |\n",
        "|    2   |        5       |      10       |     5     |     2   |\n",
        "|    3   |        5       |       20      |      1    |      3  |\n",
        "|    4   |        5       |        20     |       5   |       4 |\n",
        "|    5   |        10      |       10      | 1         |5        |\n",
        "|    6   |        10      |         10    |  5        | 6       |\n",
        "|    7   |        10      |        20     |   1       |  7      |\n",
        "|    8   |        10      |        20     |    5      |   8     |"
      ],
      "metadata": {
        "id": "6A_AtJELGPzF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WFr5dyPUHQLl"
      },
      "outputs": [],
      "source": [
        "\n",
        "np.save(path+'/Results/history_acc5.npy',np.array(history_acc))\n",
        "np.save(path+'/Results/history_recall5.npy',np.array(history_recall))\n",
        "np.save(path+'/Results/history_loss5.npy',np.array(history_loss))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "FL_SER_Group1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}